# Gihtub Copilot

Es una muy buena herramienta pero ...

1. Hay que pagar cerca de 10-12 USD al mes
2. Tu c√≥digo se puede filtrar f'acilmente
3. La nueva herramienta de microsoft `recall` va a paermitir que puedan grabars tus pantallas
4. Al final es buena opci√≥n pero existen otras opciones, ... La respuesta es s√≠

# Benchmark de LLMs para c√≥digo

Existen muchos benchmarks pero por ejemplo esta es uno bueno [EvalPlus](https://evalplus.github.io/leaderboard.html)

EvalPlus, un marco de evaluaci√≥n de s√≠ntesis de c√≥digo para evaluar rigurosamente la correcci√≥n funcional del c√≥digo sintetizado por LLMs. EvalPlus complementa un conjunto de datos de evaluaci√≥n con una gran cantidad de casos de prueba reci√©n producidos por un generador autom√°tico de entradas de prueba. El [Paper](https://openreview.net/forum?id=1qvx610Cu7)

Actualmente el benchmark muestra este ranking

1. ü•áGPT-4-Turbo (April 2024) ‚ú® ‚ö°86.6
2. ü•àGPT-4-Turbo (Nov 2023) ‚ú®‚ö°81.7
3. ü•â GPT-4 (May 2023) ‚ú®‚ö°79.3
4. CodeQwen1.5-7B-Chat ‚ú®‚ö°78.7
5. claude-3-opus (Mar 2024) ‚ú®‚ö°77.4
6. DeepSeek-Coder-33B-instruct ‚ú®‚ö°75

Si bien los mejores modelos para programar son GPT-4 justo despu√©s aparecen otras alternativas open source como `CodeQwen1.5` y `DeepSeek-Coder`. En particular `CodeQwen1.5` es un modelo de 7B adem√°s admite cerca de 92 lenguajes de programaci√≥n 

# Acoplar modelo local con tu c√≥digo

1. Deber√°s tener instalado Ollama, si no lo tienes es muy f√°cil instalarlo [Ollama](https://ollama.com/)
2. Ahora vamos a la secci√≥n de [Models](https://ollama.com/library) y buscamos `codeqwen` 
3. Abre una terminal y copia el comando
```bash
ollama run codeqwen
```
Si no tienes el modelo se descargar√°, probablemente se demore algunos minutos dependiendo de tu conexi√≥n a internet.
4. Ahora puedes mandar cualquier mensaje para ver si funciona e.g
```bash
Create a fibonacci function
```
5. Luego podr√°s salir de la interfaz con 
```bash
/exit
```
6. Ahora instalaremos una extensi√≥n de VS Code llamada `Continue`, no tiene problema con sistemas operativos.
**Nota:** Seguramente te pedir√° que inicies sesi√≥n en Github. Si no tienes una cuenta de Github ser√° mejor crear una antes de activar est√° extensi√≥n
7. Ahora deber√°s hacer click en la extensi√≥n >> `Local Models` >> `Continue`. En la parte inferior ver√°s el nombre del modelo, podr√°s elegir a `codeqwen`
8. Automaticamente podr√°s crear un mensaje y ver√°s los resultados e.g `Hello`
9. Creamos un archivo de prueba en cualquier lenguaje e.g `main.go`
10. Ahora podremos generar c√≥digo con `CTRL+I` >> Ingresa lo que quieras generar y el c√≥digo comenzar√° a generarse. Una vez terminado podr√°s aceptar o rechazar los cambios.
Tambi√©n puedes seleccionar el c√≥digo `Ctrl+May√∫s+P` y luego `CTRL+L` para a√±adir el c√≥digo al contexto
11. Y as√≠ podr√°s hacer con cualquier lenguaje de programaci√≥n
```bash
Create a sample ML project for a classification model
```

```bash
Create a simple WebUI example
```
12. **Opci√≥n de autocompletado** deber√°s hacer click en configuraci√≥n abajo a la izquierda, se abrir√° un archivo `config.json` y en la secci√≥n de Autocompletado cambiar de esto
```bash
tabAutocompleteModel": {
    "title": "Starcoder 3b",
    "provider": "ollama",
    "model": "starcoder2:3b"
  }
```
A esto:
```bash
"tabAutocompleteModel": {
    "title": "CodeQwen",
    "provider": "ollama",
    "model": "codeqwen:latest"
  },
```
Guarda los cambios y ahora podr√°s ir a tu c√≥digo y ver√°s que el autocompletado. Cool

13. Tambi√©n puedes usar cualquier modelo para el autocompletado si lo quieres por ejemplo el modelo (StarCoder)[https://ollama.com/library/starcoder2] lo √∫nico que debes hacer es descargar el modelo y utilizarlo como quieras
14. Ahora tambi√©n si no tienees una GPU podr√° susar groq como API para utilizar los modelos a trav√©s de este servicio (Groq)[https://console.groq.com/keys]
15. Luego vuelve a VS Code y da click en `+` y elige Groq, coloca tu Api Key
16. Luego puedes ir abajo y elegir `AutoDetect` y podr√°s ver los modelos de Groq
17. Tambi√©n lo puedes usar con muchos provedores sin problema

Fin!